\section{Theory  \label{section:Theory}}


\subsection{Autoencoders \label{subsection:Autoencoders}}
% Brief discription of Autoencoders
Before we describe VAEs we will give a brief overview of  Autoencoder neural networks. Autencoders are formed by the combination of two separate neural networks, and encoder network and a decoder network.  The encoder network takes as input a $d$ dimensional example $\bx$,  which it embeds into a $k$ dimensional latent representation $z$.   The decoder network then uses the latent representation $z$ to  attempt to reconstruct the original example $\hat{\bx}$. This architecture is displayed in  \textbf{Figure \ref{fig:AE}}. 
\begin{center}
    \input{docs/diagrams/AE}
    \captionof{figure}{{ \footnotesize Diagram of Autoencoder architecture for the reconstruction of an $8$ dimensional feature $\bx$ into a $3$ dimensional latent space .} \label{fig:AE}}
\end{center}
An Autoencoder is trained by minimizing the difference between its original inputs and the reconstructed output. For example, if $\bx_i$ is an input to the network, and $\Tilde{\bx}_i$ is the reconstructed output, then the squared error  reconstructed loss for  $\bx$ is 
\begin{align}
    \ell(\bx_i, \Tilde{\bx}_i) =  \|\bx_i - \Tilde{\bx}_i\|^2 \label{eq:MSEreconstructedloss}
\end{align}
\\ 

% Why they will not work for this task
Autoencoders have many useful applications.  An example of this is a  Denoising Autoencoder (DAE), which is trained to reconstruct undistorted examples after they have been slightly corrupted by some form of noise. However, vanilla Autoencoders are not always useful as a generative model as the latent embedding is not constrained to form any sort of probability distribution which would be easy to generate new examples from.\\



\subsection{Variational Autoencoders  \label{subsection:Variational Autoencoders }}

% 300 Words - Describe What an autoencoder is?
This then leads us nicely to Variational Autoencoders (VAEs), first proposed by Kingma et.al in 2014 \cite{kingma2014autoencoding}.  A VAE  has much the same architecture as a regular Autoencoder, \textbf{Figure \ref{fig:AE}}, however a VAE attempts to  learn a probability distribution over the latent space as opposed to a direct encoding. This allows new examples to be generated from the latent distribution.  \\



The basic architecture of a VAE is displayed in \textbf{Figure \ref{fig:VAE}}. An example $\bx$ is passed through a decoder network which outputs the  parameters $\mu$ and $\sigma$ of the latent distribution.  The VAE then produces a sample $\vect{z}$ from this distribution, passes the sample through the decoder network  and creates a reconstructed output $\hat{\vect{x}}$.
\begin{center}
    \input{docs/diagrams/VAE}
    \captionof{figure}{Basic Variational Autoencoder Architecture \label{fig:VAE}}
\end{center}
As was the case with regular Autoencoders, this network is trained to minimize the reconstruction loss, however, it is also trained to learn the correct latent distribution. 

\subsubsection{Probabilistic Interpretation \label{subsubection:Probalisitic Interpretation}}
We can think of a VAE as being a  probability model for our data $\vect{x}$ and the  variables $\vect{z}$  which are  generated by the latent distribution  \cite{kingma2019introduction}. 
This joint probability model $p_{\vect{\theta}}(\vect{x}, \vect{z})$ is parameterized by some vector $\vect{\theta}$ and has the factorisation
\begin{align*}
    p_{\vect{\theta}}(\vect{x}, \vect{z}) = p_{\vect{\theta}}(\vect{z}) p_{\vect{\theta}}(\vect{z}|\vect{x})
\end{align*}
The goal of a VAE is to infer the correct latent distribution given our data $\bx$ and the prior distribution that we place on $\vect{z}$,  $p_{\vect{\theta}}(\vect{z})$.  Using Bayes Theorem, the resulting posterior distribution for $\vect{z}$ is  
\begin{align}
    p_{\vect{\theta}}(\vect{z}|\vect{x}) =&  \ \frac{p_{\vect{\theta}}(\vect{z}) \nonumber p_{\vect{\theta}}(\vect{z}|\vect{x})}{p_{\vect{\theta}}(\vect{x})}\\
                        =& \  \frac{p_{\vect{\theta}}(\vect{z}) p_{\vect{\theta}}(\vect{z}|\vect{x})}{\int p_{\vect{\theta}}(\vect{x}|\vect{z})p_{\vect{\theta}}(\vect{z}) d\vect{z}} \label{eq:posterior}
\end{align}
The  quantity $p_{\vect{\theta}}(\vect{x}) =\int p(\vect{x}|\vect{z})p(\vect{z}) \ dz$ is the likelihood of our data, and in this context is also known as \textit{the evidence} for our model. As usual,  it is this quantity that we would like to maximise when learning our model parameters $\vect{\theta}$.  Directly maximising \textit{the evidence} is however intractable \cite{kingma2019introduction}, as the high dimensional integral will take exponential time to compute. As neither \textit{the evidence} or the posterior distribution are directly computable we  instead try to infer them  using the Bayesian method of variational inference. This is done by  approximating the posterior,   $p_{\vect{\theta}}(\vect{z}|\vect{x})$, using the family of  distributions $q_{\vect{\phi}}(\vect{z}|\vect{x})$, parameterized by the vector  $\vect{\phi}$. 
Using this approximate posterior, log likelihood  of \textit{the evidence}, $p_{\vect{\theta}}(\vect{x})$, can be be decomposed into  
\begin{align}
    \log p_{\vect{\theta}}(\vect{x}) =& \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}[\log p_{\vect{\theta}}(\vect{x})] \label{eq:line1}\\
    =&  \ \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log \Bigg( \frac{ p_{\vect{\theta}}(\vect{x}, \vect{z})}{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg)\Bigg] + \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log \Bigg( \frac{ q_{\vect{\phi}}(\vect{z}|\vect{x})}{p_{\vect{\theta}}(\vect{z}|\vect{x})}\Bigg)\Bigg]  \label{eq:line2}\\
    =&  \ \underbrace{\mathcal{L}(\theta, \phi)}_{\text{ELBO}} +    \underbrace{D_{KL}(q_{\vect{\phi}}(\vect{z}|\vect{x})||p_{\vect{\theta}}(\vect{z}|\vect{x}))}_{\text{KL-Divergence} \ \geq  \ 0}  \nonumber\\
    \geq &  \ \mathcal{L}(\theta, \phi) \nonumber
\end{align}
\begin{tcolorbox}[colback=blue!5,colframe=blue!75!black,title=Exercise 1]
Complete the working to get from \textbf{Equation \ref{eq:line1}} to \textbf{Equation \ref{eq:line2}}. \textbf{Solution \ref{subsection:Exercise 1}}
\end{tcolorbox}
The first term in in the RHS of \textbf{Equation \ref{eq:line2}}
\begin{align*}
   \mathcal{L}(\theta, \phi) =  \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log \Bigg( \frac{ p_{\vect{\theta}}(\vect{x}, \vect{z})}{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg)\Bigg] 
\end{align*}
is called the \textit{Variational Lower Bound}, or the \textit{Evidence Lower Bound} (ELBO), and it is  a lower bound on the quantity  $\log p_{\vect{\theta}}(\vect{x})$ which we seek to maximise. The second term on the RHS of \textbf{Equation \ref{eq:line2}}   
\begin{align*}
    D_{KL}(q_{\vect{\phi}}(\vect{z}|\vect{x})||p_{\vect{\theta}}(\vect{z}|\vect{x}))=\Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log \Bigg( \frac{ q_{\vect{\phi}}(\vect{z}|\vect{x})}{p_{\vect{\theta}}(\vect{z}|\vect{x})}\Bigg)\Bigg]
\end{align*}
is the Kullback-Leibler (KL) divergence, which is a non negative measure of how well the  distribution $q_{\vect{\phi}}(\vect{z}|\vect{x})$ approximates the true posterior  $p_{\vect{\theta}}(\vect{z}|\vect{x})$. 
Looking at \textbf{Equation \ref{eq:line1}} again, we see that if we rearrange for the ELBO $\mathcal{L}(\theta, \phi)$ we obtain
\begin{align*}
    \mathcal{L}(\theta, \phi) =& \  \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log p_{\vect{\theta}}(\bx)\Bigg] - \Em_{q_{\vect{\phi}}}\Bigg[\log q_{\vect{\phi}}(z|\bx) - \log p_{\vect{\theta}}(z|\bx)\Bigg]\\
    =&  \ \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log p_{\vect{\theta}}(\bx|z)\Bigg] - \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log q_{\vect{\phi}}(z|\bx) - \log p_{\vect{\theta}}(z)\Bigg]\\
    =&  \ \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log p_{\vect{\theta}}(\bx|z)\Bigg] - D_{KL}(q_{\vect{\phi}}(\vect{z}|\vect{x})||p_{\vect{\theta}}(\vect{z}))
\end{align*}
Hence, as the ELBO is the lower bound for $\log p_{\vect{\theta}}(\vect{x})$, maximising the ELBO for $\vect{\theta}$ and $\vect{\phi}$ is equivalent to maximising $\log p_{\vect{\theta}}(\vect{x})$ while minimising $D_{KL}(q_{\vect{\phi}}(\vect{z}|\vect{x})||p_{\vect{\theta}}(\vect{z}))$.  Here $\log p_{\vect{\theta}}(\vect{x})$ is a measure of how well we reconstruct the data, and $D_{KL}(q_{\vect{\phi}}(\vect{z}|\vect{x})||p_{\vect{\theta}}(\vect{z}))$ is a measure of how well  $q_{\vect{\phi}}(\vect{z}|\vect{x})$ fits the prior distribution $p_{\vect{\theta}}(\vect{z})$\par
% Add connections to neural networks
Taking this back into the context of VAEs as  neural networks, we can think of the approximate posterior distribution $q_{\vect{\phi}}(\vect{z}|\vect{x})$ as being the encoder network and $p_{\vect{\theta}}(\vect{x}|\vect{z})$ as decoder network for which we learn the  weights  $\vect{\phi}$ and $\vect{\theta}$.  This structure is visualised in \textbf{Figure \ref{fig:elbo}}.\par

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.5\linewidth]{docs/images/elbo.PNG}
    \caption{\cite{kingma2019introduction} Visualisation of a VAEs encoder and decoder process.}
    \label{fig:elbo}
\end{figure}


In this tutorial we are only going to consider the case where both our prior $p_{\vect{\theta}}(\vect{z})$ and posterior $q_{\vect{\phi}}(\vect{z}|\vect{x})$ are independent multivariate Guassian distributions, with $p_{\vect{\theta}}(\vect{z}) \sim \mathcal{N}(\vect{0}, \vect{I})$. We do so to obtain a closed form for the ELBO, which will be used as the loss function during the  training of the VAE.  To obtain this closed form, it can be shown that the  negative  KL divergence between $q_{\vect{\phi}}(z|\vect{x})$ and $p_{\vect{\theta}}(z)$ for a single latent random variable $z$ is  
\begin{align}
    -D_{KL}(q_{\vect{\phi}}(z|\vect{x})||p_{\vect{\theta}}(\vect{z}) =& \ -\Em_{q_{\vect{\phi}}(z|\vect{x})}\Bigg[\log \Bigg( \frac{ q_{\vect{\phi}}(z|\vect{x})}{p_{\vect{\theta}}(z)}\Bigg)\Bigg]  \nonumber\\
    =& \ \frac{1}{2}\Bigg[1 + \log(\sigma^2) - \sigma^2 - \mu \Bigg] \label{eq:kl} \  \  \ \ \cite{DBLP:journals/corr/abs-1907-08956}
\end{align}
\begin{tcolorbox}[colback=blue!5,colframe=blue!75!black,title=Exercise 2]
Show \textbf{Equation \ref{eq:kl}}. \  \  \  \textbf{Solution \ref{subsection:Exercise 2}}
\end{tcolorbox}
In this  tutorial we are attempting to reconstruct \textbf{MNIST} digits. These black and white patterns can be discretized into binary, which produces a Bernoulli likelihood for the data. If $\hat{x_i}$ is one element of a reconstructed example, then the target value $y_{i} \in \{0,1\}$ will follow the distribution $y_i \sim {\sf Ber}(\hat{x_i})$. \ 
Therefore, for a single reconstructed training example $\bx$ we will use the  Bernoulli log likelihood (Binary Cross Entropy)  to measure how well we have reconstructed the data 
\begin{align}
    \log p_{\vect{\theta}}(\bx|\vect{z}) =& \Em_{q_{\vect{\phi}}(\vect{z}|\vect{x})}\Bigg[\log p_{\vect{\theta}}(\bx|z)\Bigg] \nonumber\\
                                         \implies& \sum_{i =1}^{d} \ y_i\log(\hat{x_i}) + (1-y_i)\log(1-\hat{x_i}) \  \  \ \hat{\bx} \in (0,1)^d   \label{eq:ber}
\end{align}
Using \textbf{Equations \ref{eq:kl}} and \ref{eq:ber} we see that 
 the  our loss function, using the ELBO formulation,  for a single reconstructed example $ \hat{\bx} \in \R^d$, with target $\vect{y}$ and  latent variable  $\vect{z} \in \R^k$ will be 
\begin{align}
    \mathcal{L} =&  \underbrace{\sum_{i =1}^{d} \ y_i\log(\hat{x_i}) + (1-y_i)\log(1-\hat{x_i})}_{\text{Reconstruction }} + \underbrace{\frac{1}{2}\sum_{j = 1}^{k}\Big[1 + \log(\sigma^2_j) - \mu_j^2 - \sigma^2_j\Big]}_{\text{Divergence}}  \label{eq:loss}
\end{align}
Which is a combination of  how good the reconstruction is and how close the $q_{\vect{\phi}}(\vect{z}|\vect{x})$ approximates $p_{\vect{\theta}}(\vect{z})$.    The \mintinline{python}{python} implementation of \textbf{Equation \ref{eq:loss}} can be seen in  \textbf{Listing \ref{code:loss}}.\par


\subsubsection{Reparametrization Trick \label{subsubection:ReparametrizationTrick}}
There is one last thing that needs to be covered before we begin implementing a VAE. To allow backpropagation through our network we must employ something which is called the reparametrization trick.  The problem reparametrization fixes occurs at the sampling layer of a VAE, \textbf{Figure \ref{fig:VAE}}. If we simply generate 
\begin{align*}
    \vect{z} \sim \mathcal{N}(\vect{\mu}, \text{diag}(\vect{\sigma^2}))
\end{align*}
Then, during the networks training, backpropogation will not work correctly, as we would be attempting to take derivatives of random variables.  Using reparametrization we instead draw $\vect{\epsilon} \sim \mathcal{N}(\vect{0}, \vect{I})$ and then set 
\begin{align*}
    \vect{z} =& \vect{\mu} + \vect{\sigma} \odot \vect{\epsilon} \  \  \  \  \  \  \  
\end{align*}
 Where $\odot$ is the elementwise product. This allows use to compute $\frac{\partial \vect{z}}{\partial \vect{\mu}}$ and $\frac{\partial \vect{z}}{\partial \vect{\sigma^2}}$, as the derivatives are no longer random \cite{kingma2019introduction}.\\













